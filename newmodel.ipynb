{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "import json\n",
    "import pickle as pkl\n",
    "from pycparser import c_generator\n",
    "import ForPragmaExtractor.visitors as visitor\n",
    "from Model import tokenizer\n",
    "\n",
    "VAR_PREFIX = \"var\"\n",
    "ARR_PREFIX = \"arr\"\n",
    "FUNC_PREFIX = \"func\"\n",
    "STRUCT_PREFIX = \"struct\"\n",
    "generator = c_generator.CGenerator()\n",
    "id_v = visitor.CounterIdVisitor()\n",
    "replacer = visitor.ReplaceIdsVisitor(VAR_PREFIX, ARR_PREFIX, STRUCT_PREFIX, FUNC_PREFIX)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data1:\n",
      " {'text': [\" For: Assignment: = ID: i Constant: int, 0 BinaryOp: < ID: i ID: ni UnaryOp: p++ ID: i For: Assignment: = ID: j Constant: int, 0 BinaryOp: < ID: j ID: nl UnaryOp: p++ ID: j Assignment: = ArrayRef: ArrayRef: ID: D ID: i ID: j BinaryOp: / BinaryOp: * Cast: Typename: None, [] TypeDecl: None, [] IdentifierType: ['double'] ID: i BinaryOp: + ID: j Constant: int, 2 ID: nk\"]}\n",
      "after tokenized:\n",
      " {'input_ids': [[0, 286, 35, 46091, 35, 5457, 4576, 35, 939, 33685, 35, 6979, 6, 321, 47466, 13926, 35, 28696, 4576, 35, 939, 4576, 35, 10265, 1890, 1766, 13926, 35, 181, 42964, 4576, 35, 939, 286, 35, 46091, 35, 5457, 4576, 35, 1236, 33685, 35, 6979, 6, 321, 47466, 13926, 35, 28696, 4576, 35, 1236, 4576, 35, 295, 462, 1890, 1766, 13926, 35, 181, 42964, 4576, 35, 1236, 46091, 35, 5457, 42719, 31842, 35, 42719, 31842, 35, 4576, 35, 211, 4576, 35, 939, 4576, 35, 1236, 47466, 13926, 35, 1589, 47466, 13926, 35, 1009, 6719, 35, 5957, 9675, 4344, 35, 9291, 6, 48081, 7773, 45788, 35, 9291, 6, 48081, 28763, 24072, 40118, 35, 47052, 14582, 44403, 4576, 35, 939, 47466, 13926, 35, 2055, 4576, 35, 1236, 33685, 35, 6979, 6, 132, 4576, 35, 295, 330, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "token_len: 134 134\n",
      "vocab_size:  50265\n"
     ]
    }
   ],
   "source": [
    "def db_read_string_from_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            return \"\".join(f.readlines())   # readlines()是读全部\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def normalize_code_as_ast(pickle_file):\n",
    "    # print (pickle_file)\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        pragmafor_tuple = pkl.load(f)  #\n",
    "        for_ast = pragmafor_tuple.for_node\n",
    "        # for_ast.show()\n",
    "        # print(normalize_code_as_string.generator.visit(for_ast))\n",
    "        # for_ast.show()\n",
    "        # counts in an array the name and identifiers of the code\n",
    "        id_v.reset()\n",
    "        id_v.visit(for_ast)\n",
    "        # Replace the names..\n",
    "        replacer.reset(id_v.ids, id_v.array,id_v.struct, id_v.func)\n",
    "        replacer.visit(for_ast)\n",
    "        with open('temp.txt', 'w') as f:\n",
    "            for_ast.show(buf=f)\n",
    "        with open('temp.txt', 'r') as f:\n",
    "            ast = f.readlines()\n",
    "\n",
    "        ast_no_whitespaces = [a.strip() for a in ast] # kill all whitespaces\n",
    "        # print(ast_no_whitespaces)\n",
    "        # print(normalize_code_as_string.generator.visit(for_ast))\n",
    "\n",
    "        # print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "        ast_one_line = \" \" + \" \".join(ast_no_whitespaces)\n",
    "        return ast_one_line\n",
    "\n",
    "\n",
    "# str1 = normalize_code_as_ast('DB_TEST/DB_TEST/database/PolyBench-ACC-master_.gitignore_2mm.c_5/code_pickle.pkl')\n",
    "# print(str1)\n",
    "\n",
    "def code_as_ast(pickle_file):\n",
    "    # print (pickle_file)\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        pragmafor_tuple = pkl.load(f)  #\n",
    "        for_ast = pragmafor_tuple.for_node\n",
    "        with open('temp.txt', 'w') as f:\n",
    "            for_ast.show(buf=f)\n",
    "        with open('temp.txt', 'r') as f:\n",
    "            ast = f.readlines()\n",
    "\n",
    "        ast_no_whitespaces = [a.strip() for a in ast] # kill all whitespaces and \\n\n",
    "        ast_one_line = \" \" + \" \".join(ast_no_whitespaces)\n",
    "        return ast_one_line\n",
    "\n",
    "\n",
    "str2 = code_as_ast('D:\\CodeLibrary\\PragFormer-main\\DB_TEST\\DB_TEST\\database\\PolyBench-ACC-master_.gitignore_2mm.c_5\\code_pickle.pkl')\n",
    "# print(str2)\n",
    "# print('str2len: ', len(str2))\n",
    "\n",
    "data1 = {'text': [str2]}\n",
    "print(\"data1:\\n\", data1)\n",
    "\n",
    "text, _ = tokenizer.deepscc_tokenizer(data1['text'])\n",
    "print(\"after tokenized:\\n\", text)\n",
    "print('token_len:', len(text.input_ids[0]), len(text.attention_mask[0]))\n",
    "\n",
    "print(\"vocab_size: \", _)\n",
    "# -------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['omp parallel for private(i, j) schedule(dynamic, blocksize)', 'omp parallel for private(i, j) schedule(dynamic, blocksize)', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', '0', '0', '0', '0', '0', '0', '0', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', '0', '0', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4)', 'omp parallel for schedule(dynamic,4)', 'omp parallel for schedule(dynamic,4)', 'omp parallel for schedule(dynamic,4)', 'omp parallel for schedule(dynamic,4)', 'omp parallel for schedule(dynamic,4)', 'omp parallel for schedule(dynamic,4)', 'omp parallel for schedule(dynamic,4)', 'omp parallel for schedule(dynamic,4)', 'omp parallel for schedule(dynamic,4)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4)', 'omp parallel for schedule(dynamic,4)', 'omp parallel for schedule(dynamic,4)', 'omp parallel for schedule(dynamic,4)', 'omp parallel for schedule(dynamic,4)', 'omp parallel for schedule(dynamic,4)', 'omp parallel for schedule(dynamic,4)', 'omp parallel for schedule(dynamic,4)', 'omp parallel for schedule(dynamic,4)', 'omp parallel for schedule(dynamic,4) shared(status)', '0', '0', '0', '0', '0', '0', '0', '0', 'omp parallel for schedule(dynamic,4) shared(status)', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', 'omp parallel for schedule(dynamic,4) shared(progress,status) omp_throttle(1)', 'omp parallel for schedule(dynamic,4) shared(progress,status) omp_throttle(1)', '0', '0', '0', '0', '0', '0', '0', '0', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', 'omp parallel for schedule(dynamic,4)', 'omp parallel for schedule(dynamic,4) shared(progress,status)', 'omp parallel for schedule(dynamic,4)', 'omp parallel for schedule(dynamic,4) shared(progress,status)', 'omp parallel for schedule(dynamic,4) shared(progress,status)', 'omp parallel for schedule(dynamic,4) shared(progress,status)', 'omp parallel for schedule(dynamic,4) shared(progress,status)', 'omp parallel for schedule(dynamic,4)', 'omp parallel for schedule(dynamic,4) shared(progress,status)', 'omp parallel for schedule(dynamic,4) shared(progress,status)', 'omp parallel for schedule(dynamic,4) shared(progress,status)', 'omp parallel for schedule(dynamic,4) shared(progress,status)', 'omp parallel for schedule(dynamic,4) shared(progress,status)', 'omp parallel for schedule(dynamic,4) shared(progress,status)', 'omp parallel for schedule(dynamic,4) shared(progress,status)', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', 'omp parallel for schedule(dynamic,4) shared(status) omp_throttle(1)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', '0', 'omp parallel for schedule(dynamic,4) shared(progress,status)', 'omp parallel for schedule(dynamic,4) shared(status)', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', 'omp parallel for schedule(dynamic,4)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(dynamic,4) shared(status)', 'omp parallel for schedule(static,1) shared(progress, status) omp_throttle(1)', 'omp parallel for schedule(static,8) shared(progress,status) omp_throttle(1)', 'omp parallel for schedule(static,1) shared(progress,status) omp_throttle(1)', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', 'omp parallel for schedule(dynamic,4) shared(status)', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', 'omp parallel for private (i)', 'omp parallel for private (i)', 'omp parallel for private (j)', 'omp parallel for private (j2, i)', '0', '0', 'omp parallel for private (i)', 'omp parallel for private (j)', 'omp parallel for private (j2, i)', '0', '0', 'omp parallel for private (j, k)', 'omp parallel for private (j, k)', '0', '0', '0', '0', '0', 'omp parallel for private(k)', 'omp parallel for private(j,k)', 'omp parallel for', '0', '0', '0', '0', '0', 'omp parallel for', 'omp parallel for private (j)', '0', '0', '0', 'omp parallel for', 'omp parallel for private (j)', '0', '0', '0', '0', 'omp parallel for private (j,k)', '0', '0', 'omp parallel for private (q, p, s)', '0', '0', '0', 'omp parallel for private (j, k)', '0', '0', '0', '0', 'omp parallel for private (j)', 'omp parallel for private (j)', 'omp parallel for', 'omp parallel for private (j)', '0', '0', 'omp parallel for private (j)', '0', '0', 'omp parallel for', 'omp parallel for', '0', '0', 'omp parallel for private(j,acc,k)', '0', '0', '0', 'omp parallel for private (j) schedule(static)', 'omp parallel for private (j, k) schedule(static)', '0', '0', '0', 'omp parallel for private(j)', 'omp parallel for private(j,k)', '0', '0', '0', 'omp parallel for schedule(static)', '0', '0', 'omp parallel for private (j, k)', '0', '0', 'omp parallel for private (i)', 'omp parallel for', '0', '0', 'omp parallel for private (j)', 'omp parallel for private (j, k)', '0', 'omp parallel for private (i, j)', '0', '0', '0', '0', '0', 'omp parallel for private (j, i)', '0', '0', 'omp parallel for private (j, w)', 'omp parallel for private (j, w)', '0', '0', '0', 'omp parallel for private (i, cnt) collapse(2) schedule(static)', 'omp parallel for private (i, cnt) collapse(2) schedule(static)', 'omp parallel for', 'omp parallel for private (i) collapse(2) schedule(static)', '0', '0', 'omp parallel for', 'omp parallel for', 'omp parallel for', 'omp parallel for', 'omp parallel for', 'omp parallel for', '0', '0', 'omp parallel for private(j) collapse(2) schedule(static)', '0', '0', 'omp parallel for private (j,k) collapse(2)', '0', '0', 'omp parallel for', 'omp parallel for private(i,j) collapse(2) schedule(static)', 'omp parallel for private(i,j) collapse(2) schedule(static)', 'omp parallel for private(i,j) collapse(2) schedule(static)', '0', '0', '0', 'omp parallel for private (iy, ix)', '0', '0', '0', '0', '0', 'omp parallel for schedule(static)', 'omp parallel for schedule(static)', '0', '0', 'omp parallel for schedule(static)', 'omp parallel for schedule(static)', '0', '0', 'omp parallel for private(i,j,t) schedule(static) collapse (2)', '0', '0', 'omp parallel for']\n",
      "['2', '2', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '1', '1', '1', '0', '0', '0', '0', '0', '0', '0', '1', '1', '1', '0', '0', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '1', '1', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '1', '1', '1', '1', '0', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '1', '1', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '2', '2', '2', '2', '0', '0', '2', '2', '2', '0', '0', '2', '2', '0', '0', '0', '0', '0', '2', '2', '1', '0', '0', '0', '0', '0', '1', '2', '0', '0', '0', '1', '2', '0', '0', '0', '0', '2', '0', '0', '2', '0', '0', '0', '2', '0', '0', '0', '0', '2', '2', '1', '2', '0', '0', '2', '0', '0', '1', '1', '0', '0', '2', '0', '0', '0', '2', '2', '0', '0', '0', '2', '2', '0', '0', '0', '1', '0', '0', '2', '0', '0', '2', '1', '0', '0', '2', '2', '0', '2', '0', '0', '0', '0', '0', '2', '0', '0', '2', '2', '0', '0', '0', '2', '2', '1', '2', '0', '0', '1', '1', '1', '1', '1', '1', '0', '0', '2', '0', '0', '2', '0', '0', '1', '2', '2', '2', '0', '0', '0', '2', '0', '0', '0', '0', '0', '1', '1', '0', '0', '1', '1', '0', '0', '2', '0', '0', '1']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    数据处理\n",
    "\"\"\"\n",
    "data_set = {'text':[], 'label':[], 'ast':[]}\n",
    "jsonpath = 'D:\\CodeLibrary\\PragFormer-main\\DB_TEST\\DB_TEST\\database.json'\n",
    "with open(jsonpath, 'r') as f:\n",
    "    file_data = json.load(f)\n",
    "    for i, key in enumerate(file_data):\n",
    "        code = db_read_string_from_file(file_data[key][\"code\"])\n",
    "        ast_str = code_as_ast(file_data[key]['code_pickle'])\n",
    "        if file_data[key]['pragma']:\n",
    "            pragma = db_read_string_from_file(file_data[key]['pragma'])\n",
    "        else:\n",
    "            pragma = '0'\n",
    "\n",
    "        data_set['text'].append(code)\n",
    "        data_set['ast'].append(ast_str)\n",
    "        data_set['label'].append(pragma)\n",
    "\n",
    "# print(\"data_set['text'][0]\\n\", data_set['text'][0])\n",
    "# print(\"data_set['text'][10]\\n\", data_set['text'][10])\n",
    "# print(\"data_set['ast'][0]\\n\", data_set['ast'][0])\n",
    "# print(\"data_set['ast'][10]\\n\", data_set['ast'][10])\n",
    "# print(\"data_set['label'][0]\\n\", data_set['label'][0])\n",
    "# print(\"data_set['label'][10]\\n\", data_set['label'][10])\n",
    "\n",
    "def label_encoder(labels:list):\n",
    "    new_labels = []\n",
    "    for label in labels:\n",
    "        if 'omp parallel for' in label and 'private' not in label and 'reduction' not in label:\n",
    "            new_labels.append('1')\n",
    "        elif 'omp parallel for' in label and 'private' in label and 'reduction' not in label:\n",
    "            new_labels.append('2')\n",
    "        elif 'omp parallel for' in label and 'reduction' in label and 'private' not in label:\n",
    "            new_labels.append('3')\n",
    "        elif 'omp parallel for' in label and 'reduction' in label and 'private' in label:\n",
    "            new_labels.append(\"4\")\n",
    "        else:\n",
    "            new_labels.append(\"0\")\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     return {\n",
    "#         'input_ids': pad_sequence([item['input_ids'] for item in batch], batch_first=True),\n",
    "#         'attention_mask': pad_sequence([item['attention_mask'] for item in batch], batch_first=True),\n",
    "#         'labels': torch.tensor([item['labels'] for item in batch])\n",
    "#     }\n",
    "#\n",
    "print(data_set['label'])\n",
    "data_set['label'] = label_encoder(data_set['label'])\n",
    "print(data_set['label'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "texts_train, texts_test, asts_train, asts_test, labels_train, labels_test = train_test_split(\n",
    "    data_set['text'], data_set['ast'], data_set['label'], test_size=0.2, random_state=42\n",
    ") \n",
    "\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, texts, asts, labels):\n",
    "        self.texts = texts\n",
    "        self.asts = asts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        ast = self.asts[idx]\n",
    "        label = int(self.labels[idx])  # Assuming labels are numerical\n",
    "\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(text, ast, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "\n",
    "# Create dataset and dataloaders for training and testing\n",
    "train_dataset = CodeDataset(texts_train, asts_train, labels_train)\n",
    "test_dataset = CodeDataset(texts_test, asts_test, labels_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 46.00 MiB. GPU 0 has a total capacty of 8.00 GiB of which 0 bytes is free. Of the allocated memory 7.26 GiB is allocated by PyTorch, and 43.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Move model to GPU if available\u001b[39;00m\n\u001b[0;32m     11\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Define the optimizer\u001b[39;00m\n\u001b[0;32m     16\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m)\n",
      "File \u001b[1;32me:\\software\\anaconda3\\envs\\pragformer_env\\lib\\site-packages\\transformers\\modeling_utils.py:1902\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1897\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1898\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.to` is not supported for `4-bit` or `8-bit` models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1899\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1900\u001b[0m     )\n\u001b[0;32m   1901\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\software\\anaconda3\\envs\\pragformer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\software\\anaconda3\\envs\\pragformer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32me:\\software\\anaconda3\\envs\\pragformer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32me:\\software\\anaconda3\\envs\\pragformer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32me:\\software\\anaconda3\\envs\\pragformer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32me:\\software\\anaconda3\\envs\\pragformer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 46.00 MiB. GPU 0 has a total capacty of 8.00 GiB of which 0 bytes is free. Of the allocated memory 7.26 GiB is allocated by PyTorch, and 43.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "model = model.half()\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0  # Track total training loss for the epoch\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Release GPU cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "# Validation loop (optional)\n",
    "    model.eval()\n",
    "    total_test_loss = 0.0  # Track total testing loss for the epoch\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "            # Get model predictions\n",
    "            logits = outputs.logits\n",
    "            _, predicted_labels = torch.max(logits, dim=1)\n",
    "            predictions.extend(predicted_labels.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate average testing loss for the epoch\n",
    "    avg_test_loss = total_test_loss / len(test_loader)\n",
    "\n",
    "    # Calculate accuracy on the testing set\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Testing Loss: {avg_test_loss:.4f}, Testing Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pragformer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
